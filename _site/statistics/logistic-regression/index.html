<!DOCTYPE html>
<!--
    So Simple Jekyll Theme 3.2.0
    Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
    Free for personal and commercial use under the MIT license
    https://github.com/mmistakes/so-simple-theme/blob/master/LICENSE
-->
<html lang="en-US" class="no-js">
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  

  
    
    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Logistic Regression | Stochastic Entity</title>
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="Logistic Regression" />
<meta name="author" content="Vikrant Kamble" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="One of the most common test case in supervised machine learning is that of classification:" />
<meta property="og:description" content="One of the most common test case in supervised machine learning is that of classification:" />
<link rel="canonical" href="http://localhost:4000/statistics/logistic-regression/" />
<meta property="og:url" content="http://localhost:4000/statistics/logistic-regression/" />
<meta property="og:site_name" content="Stochastic Entity" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-11-12T09:23:00-08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Logistic Regression" />
<meta name="twitter:site" content="@" />
<meta name="twitter:creator" content="@Vikrant Kamble" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Vikrant Kamble"},"dateModified":"2019-11-12T09:23:00-08:00","datePublished":"2019-11-12T09:23:00-08:00","description":"One of the most common test case in supervised machine learning is that of classification:","headline":"Logistic Regression","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/statistics/logistic-regression/"},"url":"http://localhost:4000/statistics/logistic-regression/"}</script>
<!-- End Jekyll SEO tag -->


  

  <script>
    /* Cut the mustard */
    if ( 'querySelector' in document && 'addEventListener' in window ) {
      document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + 'js';
    }
  </script>

  <link rel="stylesheet" href="https://use.typekit.net/eio0pzn.css">

  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="stylesheet" href="/assets/css/skins/default.css">
  
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=EB+Garamond:400,400i,700,700i|Lora:400,400i,700,700i">
  <link rel="alternate" type="application/atom+xml" title="Stochastic Entity" href="/atom.xml">
<!-- start custom head snippets -->

<!-- insert favicons. use http://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

</head>


  <body class="layout--post  logistic-regression">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#primary-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    
  <div class="navigation-wrapper">
    <a href="#menu-toggle" id="menu-toggle">Menu</a>
    <nav id="primary-nav" class="site-nav animated drop">
      <ul><li><a href="/posts">Home</a></li><li><a href="/categories/">Categories</a></li><li><a href="/tags/">Tags</a></li><li><a href="/search/">Search</a></li></ul>
    </nav>
  </div><!-- /.navigation-wrapper -->


    <header class="masthead">
  <div class="wrap">
    
    
    
      
        <div class="site-title animated fadeIn"><a href="/">Stochastic Entity</a></div>
      
      <p class="site-description animated fadeIn" itemprop="description">Personal blog for ideas and docs.</p>
    
  </div>
</header><!-- /.masthead -->


    <main id="main" class="main-content" aria-label="Content">
  <article class="h-entry">
    

    <div class="page-wrapper">
      <header class="page-header">
        
        
          <h1 id="page-title" class="page-title p-name">Logistic Regression
</h1>
        
      </header>

      <div class="page-sidebar">
        <div class="page-author h-card p-author"><img src="/images/headshot_compressed.jpg" class="author-avatar u-photo" alt="Vikrant Kamble"><div class="author-info"><div class="author-name">
        <em>by</em> <span class="p-name">Vikrant Kamble</span>
      </div><ul class="author-links"><li class="author-link">
            <a class="u-url" rel="me" href="https://instagram.com/vikrant10622"><i class="fab fa-instagram fa-lg" title="Instagram"></i></a>
          </li><li class="author-link">
            <a class="u-url" rel="me" href="https://github.com/VikrantKamble"><i class="fab fa-github-square fa-lg" title="GitHub"></i></a>
          </li></ul>
    <time class="page-date dt-published" datetime="2019-11-12T09:23:00-08:00"><a class="u-url" href="">November 12, 2019</a>
</time>

  </div>
</div>

        
  <h3 class="page-taxonomies-title">Categories</h3>
  
  <ul class="page-taxonomies"><li class="page-taxonomy">statistics</li>
  </ul>


        

      </div>

      <div class="page-content">
        <div class="e-content">
          <p>One of the most common test case in supervised machine learning is that of classification:</p>

<blockquote>
  <p>Given a data point, classify it into one of the many labels available</p>
</blockquote>

<p>The simplest case of this is binary classification. For e.g. classifying emails as spam or ham,
classifying objects as star or galaxy, classifying images as a cat or a dog, etc. In all these cases, to be able to apply the
mathematical modeling of machine learning one needs to <em>abstract</em> away some <em>feature(s)</em> of the object. Most often choosing or engineering the right features is what gives more power to predictive modeling rather than sophisticated models.</p>

<blockquote>
If your features are crappy, no amount of sophisticated models can come to the rescue.
</blockquote>

<p>However, in this post we are going to assume that we already have the features in hand. Given this, there are broadly two different ways of approaching classification problems - parametric and non-parametric. The topic we are going to discuss falls in the non-parametric approaches; more specifically as a linear model. It creates a linear boundary separating objects of one class from the other given as:</p>

\[y = sign(x'^T w' + w_0) = x^T w, \qquad y \in \{-1, 1\}.\]

<p>Conceptually, one can also think of the boundary as a set of points in the feature space whose probability of belonging to class $y=1$ (or $y=-1$) is half. As we move away from the boundary, on one side the probability of belonging to $y=1$ diminishes, while on the other end it approaches 1.</p>

<p align="center">
  <img src="/static/img/log_reg_eg.png" width="400" />
</p>

<blockquote>
  <p>Mathematically, what we want is a <em>specific</em> function of the perpendicular distance of any point from the boundary given by $d_\perp = x^T w$, whose value on the boundary be 0.5 and should range from 0 to 1 as we move from one side of the boundary to the other.</p>
</blockquote>

<p>Given these properties, the value of this function for any point can be interpreted as its probability of belonging to class $y=1$. There are many such functions that we could choose as shown below.</p>

<p align="center">
  <img src="/static/img/link_function.png" width="600" />
</p>

<p>The functional form used by Logistic regression is:</p>

\[\mathrm{Prob}[x \in (y=1)] = f(d_\perp) = \frac{e^{d_\perp}}{1 + e^{d_\perp}} = \frac{e^{x^T w}}{1 + e^{x^T w}}\]

<hr />

<p>One common terminology used in this regards is that of <em>Odds</em>. It is defined as:</p>

\[Odds = \frac{P(y = 1)}{P(y = -1)} = \frac{P(y = 1)}{1 - P(y = 1)}\]

<p>The domain of Odds lies in the range $[0, \infty)$. The natural logarithm of the above quantity is called <em>Log Odds</em> which lies in the range $(-\infty, \infty)$. This can be shown to be</p>

\[Log\ Odds = x^T w\]

<p>Thus we can interpret coefficient corresponding to a given predicting variable as: <strong>the change in log odds for a unit change in a given variable keeping the rest variables fixed.</strong></p>

<hr />

<p>Our goal in now to estimate the value of the parameters of the model, $w$, from training data. However, to assess which values of the parameters are <em>good</em>, we have to first define the likelihood. Under the assumption that each data-point is independent of every other data-point, we have:</p>

\[\mathcal{L} = \prod_{i=1}^n p(y_i | x_i, w) = \prod_{i=1}^n [\mathcal{I}(y_i = 1) f(d_\perp^i)][\mathcal{I}(y_i = -1) 1 - f(d_\perp^i)]\]

<p>Remember, $f(d_\perp)$ gives the probability of belonging to class $y=1$. So if a data-point belongs to $y=-1$ we need to use $1 - f(d_\perp)$.</p>

<p>
$$\begin{align} \mathcal{L} = \prod_{i=1}^n p(y_i | x_i, w) &amp;= \prod_{i=1}^n \left[\mathcal{I}(y_i = 1) \frac{\displaystyle e^{x_i^T w}}{1 + e^{x_i^T w}} \right] \left[\mathcal{I}(y_i = -1) \left(1 - \frac{\displaystyle e^{x_i^T w}}{1 + e^{x_i^T w}} \right) \right] \\\\
&amp;= \prod_{i=1}^n \frac{e^{y_i x_i^T w}}{1 + e^{y_i x_i^T w}}\end{align}$$
</p>

<p>Writing in terms of log-likelihood this becomes:</p>

<p>
$$ \begin{align} \log\ \mathcal{L} &amp;= \sum_{i=1}^n \log [e^{y_i x_i^T w}] - \sum_{i=1}^n \log [1 + e^{y_i x_i^T w}] \\\\
&amp;= \sum_{i=1}^n y_i x_i^T w - \sum_{i=1}^n \log [1 + e^{y_i x_i^T w}] \end{align}$$
</p>

<p>The value of $w$ we are looking for them becomes:</p>

\[w_{ML} = \underset{w}{\mathrm{argmax}} \log\ \mathcal{L}\]

<p>However, there is no analytical solution for $w$ given the form of the likelihood. Thatâ€™s where <a href="https://en.wikipedia.org/wiki/Gradient_descent">gradient descent</a> comes to the rescue. We first randomly initialize $w$ and iteratively update it till a convergence criterion is reached.</p>

\[w_{t+1} = w_t + \eta\ \nabla_w \log \mathcal{L},\]

<p>where $\eta$ is the learning rate. The gradient in the above equation is easy to compute, given by:</p>

\[\nabla_w\ \log \mathcal{L} = \sum_i^n y_i x_i \left[\frac{1}{1 + e^{y_i x_i^T w}}\right].\]

<p>More often that not, using the maximum likelihood prescription as above leads to estimates that have high variance. Especially in the case when one has fewer data-points than that is representative of the population, the classifier will be able to clearly separate the two classes with a linear boundary, with no misclassified points. In that case the value of $w$ will blow up. Points on either side of the boundary will be classified as belonging to their respective classes with probability of 1. This however may not generalize to new data-points and lead to a classifier with high variance. <em>You basically <strong>read</strong> too much into your data, leading to overfitting.</em></p>

<p>This can be prevented using <a href="https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a">regularization</a>. To keep the estimated $w$ from becoming too large, one can add a penalty term to the likelihood.</p>

\[w_{reg} = \underset{w}{\mathrm{argmax}} \log\ \mathcal{L} - \lambda ||w||^2 \qquad \mathrm{Ridge}\]

\[w_{reg} = \underset{w}{\mathrm{argmax}} \log\ \mathcal{L} - \lambda ||w||_1 \qquad \mathrm{Lasso}\]


        </div>

        
          <div class="page-share">
  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2Fstatistics%2Flogistic-regression%2F" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" class="btn btn--facebook btn--small"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i> <span>Share</span></a>
  <a href="https://twitter.com/intent/tweet?text=Logistic+Regression%20http%3A%2F%2Flocalhost%3A4000%2Fstatistics%2Flogistic-regression%2F" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" class="btn btn--twitter btn--small"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i> <span>Tweet</span></a>
  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2Fstatistics%2Flogistic-regression%2F" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" class="btn btn--linkedin btn--small"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> <span>LinkedIn</span></a>
  <a href="https://reddit.com/submit?title=Logistic+Regression&url=http%3A%2F%2Flocalhost%3A4000%2Fstatistics%2Flogistic-regression%2F" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" class="btn btn--reddit btn--small"><i class="fab fa-fw fa-reddit" aria-hidden="true"></i> <span>Reddit</span></a>
</div>

        

        

        <nav class="page-pagination" role="navigation">
  
    <a class="page-previous" href="/statistics/poisson-process/">
      <h4 class="page-pagination-label">Previous</h4>
      <span class="page-pagination-title">
        <i class="fas fa-arrow-left"></i> Poisson Process

      </span>
    </a>
  

  
    <a class="page-next" href="/statistics/absorbing-markov-chain/">
      <h4 class="page-pagination-label">Next</h4>
      <span class="page-pagination-title">
        Absorbing Markov Chains
 <i class="fas fa-arrow-right"></i>
      </span>
    </a>
  
</nav>

      </div>
    </div>
  </article>
</main>


    <footer id="footer" class="site-footer">
  <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
<div class="social-icons"><a class="social-icon" href="https://github.com/VikrantKamble"><i class="fab fa-github-square fa-2x" title="GitHub"></i></a></div><div class="copyright">
    
      <p>&copy; 2024 Stochastic Entity. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://github.com/mmistakes/so-simple-theme" rel="nofollow">So Simple</a>.</p>
    
  </div>
</footer>

    <script src="https://code.jquery.com/jquery-3.3.1.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script>
  <script src="/assets/js/main.min.js"></script>
  <script src="https://use.fontawesome.com/releases/v5.0.12/js/all.js"></script>

<!-- for mathjax support --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$', '$'] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      messageStyle: "none",
      "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
    });
    </script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  </body>

</html>
